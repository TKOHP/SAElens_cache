{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6UyzFGfBSdA"
   },
   "source": [
    "# Understanding SAE Features with the Logit Lens\n",
    "\n",
    "This notebook demonstrates how to use the mats_sae_training library to perform the analysis documented the post \"[Understanding SAE Features with the Logit Lens](https://www.alignmentforum.org/posts/qykrYY6rXXM7EEs8Q/understanding-sae-features-with-the-logit-lens)\".\n",
    "\n",
    "As such, the notebook will include sections for:\n",
    "- Loading in GPT2-Small Residual Stream SAEs from Huggingface.\n",
    "- Performing Virtual Weight Based Analysis of features (specifically looking at the logit weight distributions).\n",
    "- Programmatically opening neuronpedia tabs to engage with public dashboards on [neuronpedia](https://www.neuronpedia.org/).\n",
    "- Performing Token Set Enrichment Analysis (based on Gene Set Enrichment Analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff-PciWXBSdB"
   },
   "source": [
    "## Set Up\n",
    "\n",
    "Here we'll load various functions for things like:\n",
    "- downloading and loading our SAEs from huggingface.\n",
    "- opening neuronpedia from a jupyter cell.\n",
    "- calculating statistics of the logit weight distributions.\n",
    "- performing Token Set Enrichment Analysis (TSEA) and plotting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vImmTg-8BSdC"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sOd2C0e1BfN1",
    "ExecuteTime": {
     "end_time": "2024-07-29T01:13:43.998818600Z",
     "start_time": "2024-07-29T01:13:43.987818500Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # For Google Colab, a high RAM instance is needed\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QmdAd_25BSdC",
    "ExecuteTime": {
     "end_time": "2024-07-29T01:23:23.332321900Z",
     "start_time": "2024-07-29T01:23:23.299315Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import plotly_express as px\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Model Loading\n",
    "from sae_lens import SAE\n",
    "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "# Virtual Weight / Feature Statistics Functions\n",
    "from sae_lens.analysis.feature_statistics import (\n",
    "    get_all_stats_dfs,\n",
    "    get_W_U_W_dec_stats_df,\n",
    ")\n",
    "\n",
    "# Enrichment Analysis Functions\n",
    "from sae_lens.analysis.tsea import (\n",
    "    get_enrichment_df,\n",
    "    manhattan_plot_enrichment_scores,\n",
    "    plot_top_k_feature_projections_by_token_and_category,\n",
    ")\n",
    "from sae_lens.analysis.tsea import (\n",
    "    get_baby_name_sets,\n",
    "    get_letter_gene_sets,\n",
    "    generate_pos_sets,\n",
    "    get_test_gene_sets,\n",
    "    get_gene_set_from_regex,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VEAe5FjBSdD"
   },
   "source": [
    "### Loading GPT2 Small and SAE Weights\n",
    "\n",
    "This will take a while the first time you run it, but will be quick thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HQ904zDOBSdD",
    "ExecuteTime": {
     "end_time": "2024-07-29T01:23:55.553199800Z",
     "start_time": "2024-07-29T01:23:44.945707Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like gpt2 is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:196\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 196\u001B[0m     sock \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_connection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dns_host\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43msource_address\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msource_address\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43msocket_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 85\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     72\u001B[0m     sock\u001B[38;5;241m.\u001B[39mbind(source_address)\n\u001B[0;32m---> 73\u001B[0m \u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43msa\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "\u001B[0;31mOSError\u001B[0m: [Errno 101] Network is unreachable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mNewConnectionError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:490\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    489\u001B[0m         new_e \u001B[38;5;241m=\u001B[39m _wrap_proxy_error(new_e, conn\u001B[38;5;241m.\u001B[39mproxy\u001B[38;5;241m.\u001B[39mscheme)\n\u001B[0;32m--> 490\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m new_e\n\u001B[1;32m    492\u001B[0m \u001B[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001B[39;00m\n\u001B[1;32m    493\u001B[0m \u001B[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    465\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 466\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1095\u001B[0m, in \u001B[0;36mHTTPSConnectionPool._validate_conn\u001B[0;34m(self, conn)\u001B[0m\n\u001B[1;32m   1094\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_closed:\n\u001B[0;32m-> 1095\u001B[0m     \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:615\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    614\u001B[0m sock: socket\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m|\u001B[39m ssl\u001B[38;5;241m.\u001B[39mSSLSocket\n\u001B[0;32m--> 615\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m sock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    616\u001B[0m server_hostname: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:211\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 211\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NewConnectionError(\n\u001B[1;32m    212\u001B[0m         \u001B[38;5;28mself\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to establish a new connection: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    213\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001B[39;00m\n",
      "\u001B[0;31mNewConnectionError\u001B[0m: <urllib3.connection.HTTPSConnection object at 0x7ff115d8bd60>: Failed to establish a new connection: [Errno 101] Network is unreachable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mMaxRetryError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:843\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    841\u001B[0m     new_e \u001B[38;5;241m=\u001B[39m ProtocolError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection aborted.\u001B[39m\u001B[38;5;124m\"\u001B[39m, new_e)\n\u001B[0;32m--> 843\u001B[0m retries \u001B[38;5;241m=\u001B[39m \u001B[43mretries\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mincrement\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_e\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_stacktrace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexc_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    846\u001B[0m retries\u001B[38;5;241m.\u001B[39msleep()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/retry.py:519\u001B[0m, in \u001B[0;36mRetry.increment\u001B[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[1;32m    518\u001B[0m     reason \u001B[38;5;241m=\u001B[39m error \u001B[38;5;129;01mor\u001B[39;00m ResponseError(cause)\n\u001B[0;32m--> 519\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MaxRetryError(_pool, url, reason) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mreason\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    521\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncremented Retry for (url=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m): \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, url, new_retry)\n",
      "\u001B[0;31mMaxRetryError\u001B[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /gpt2/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff115d8bd60>: Failed to establish a new connection: [Errno 101] Network is unreachable'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1751\u001B[0m, in \u001B[0;36m_get_metadata_or_catch_error\u001B[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001B[0m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m \u001B[43mget_hf_file_metadata\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1752\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\n\u001B[1;32m   1753\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1673\u001B[0m, in \u001B[0;36mget_hf_file_metadata\u001B[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001B[0m\n\u001B[1;32m   1672\u001B[0m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[0;32m-> 1673\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1674\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHEAD\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1675\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1676\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1677\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1678\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1679\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1680\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1681\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1682\u001B[0m hf_raise_for_status(r)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:376\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[0;32m--> 376\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[1;32m    384\u001B[0m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:399\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001B[39;00m\n\u001B[0;32m--> 399\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mget_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    400\u001B[0m hf_raise_for_status(response)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:66\u001B[0m, in \u001B[0;36mUniqueRequestIdAdapter.send\u001B[0;34m(self, request, *args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:700\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    698\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m SSLError(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[0;32m--> 700\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ClosedPoolError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mConnectionError\u001B[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /gpt2/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7ff115d8bd60>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: a6a33547-6275-42ac-864c-5c55f43c4299)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mLocalEntryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    400\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    401\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 402\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    405\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    406\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    408\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    409\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001B[0m, in \u001B[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    100\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[0;32m--> 101\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1240\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001B[0m\n\u001B[1;32m   1239\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1241\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[1;32m   1242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1243\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[1;32m   1244\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1245\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1246\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1248\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[1;32m   1249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1250\u001B[0m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1253\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1254\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[1;32m   1255\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1256\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1257\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1347\u001B[0m, in \u001B[0;36m_hf_hub_download_to_cache_dir\u001B[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[0m\n\u001B[1;32m   1346\u001B[0m     \u001B[38;5;66;03m# Otherwise, raise appropriate error\u001B[39;00m\n\u001B[0;32m-> 1347\u001B[0m     \u001B[43m_raise_on_head_call_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhead_call_error\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1349\u001B[0m \u001B[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1857\u001B[0m, in \u001B[0;36m_raise_on_head_call_error\u001B[0;34m(head_call_error, force_download, local_files_only)\u001B[0m\n\u001B[1;32m   1855\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1856\u001B[0m     \u001B[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001B[39;00m\n\u001B[0;32m-> 1857\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LocalEntryNotFoundError(\n\u001B[1;32m   1858\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1859\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1860\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is on.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1861\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhead_call_error\u001B[39;00m\n",
      "\u001B[0;31mLocalEntryNotFoundError\u001B[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# # Add domestic mirror path\u001B[39;00m\n\u001B[1;32m      3\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHF_ENDPOINT\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://hf-mirror.com\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 5\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mHookedTransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgpt2-small\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# this is an outdated way to load the SAE. We need to have feature spartisity loadable through the new interface to remove it.\u001B[39;00m\n\u001B[1;32m      7\u001B[0m gpt2_small_sparse_autoencoders \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m/root/data/miniconda3/miniconda3/envs/SAE_Lens/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:1246\u001B[0m, in \u001B[0;36mHookedTransformer.from_pretrained\u001B[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, **from_pretrained_kwargs)\u001B[0m\n\u001B[1;32m   1242\u001B[0m official_model_name \u001B[38;5;241m=\u001B[39m model_name\n\u001B[1;32m   1243\u001B[0m \u001B[38;5;66;03m# Load the config into an HookedTransformerConfig object. If loading from a\u001B[39;00m\n\u001B[1;32m   1244\u001B[0m \u001B[38;5;66;03m# checkpoint, the config object will contain the information about the\u001B[39;00m\n\u001B[1;32m   1245\u001B[0m \u001B[38;5;66;03m# checkpoint\u001B[39;00m\n\u001B[0;32m-> 1246\u001B[0m cfg \u001B[38;5;241m=\u001B[39m \u001B[43mloading\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_pretrained_model_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1247\u001B[0m \u001B[43m    \u001B[49m\u001B[43mofficial_model_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1248\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhf_cfg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_cfg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1249\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheckpoint_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheckpoint_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1250\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheckpoint_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheckpoint_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1251\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfold_ln\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfold_ln\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_devices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_devices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdefault_prepend_bos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_prepend_bos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1256\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfrom_pretrained_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1257\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1259\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cfg\u001B[38;5;241m.\u001B[39mpositional_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshortformer\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1260\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fold_ln:\n",
      "File \u001B[0;32m/root/data/miniconda3/miniconda3/envs/SAE_Lens/lib/python3.10/site-packages/transformer_lens/loading_from_pretrained.py:1425\u001B[0m, in \u001B[0;36mget_pretrained_model_config\u001B[0;34m(model_name, hf_cfg, checkpoint_index, checkpoint_value, fold_ln, device, n_devices, default_prepend_bos, dtype, **kwargs)\u001B[0m\n\u001B[1;32m   1421\u001B[0m         logging\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m   1422\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mofficial_model_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires setting trust_remote_code=True\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1423\u001B[0m         )\n\u001B[1;32m   1424\u001B[0m         kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrust_remote_code\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m-> 1425\u001B[0m     cfg_dict \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_hf_model_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mofficial_model_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# Processing common to both model types\u001B[39;00m\n\u001B[1;32m   1427\u001B[0m \u001B[38;5;66;03m# Remove any prefix, saying the organization who made a model.\u001B[39;00m\n\u001B[1;32m   1428\u001B[0m cfg_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_name\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m official_model_name\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m/root/data/miniconda3/miniconda3/envs/SAE_Lens/lib/python3.10/site-packages/transformer_lens/loading_from_pretrained.py:718\u001B[0m, in \u001B[0;36mconvert_hf_model_config\u001B[0;34m(model_name, **kwargs)\u001B[0m\n\u001B[1;32m    716\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    717\u001B[0m     huggingface_token \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHF_TOKEN\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 718\u001B[0m     hf_config \u001B[38;5;241m=\u001B[39m \u001B[43mAutoConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    719\u001B[0m \u001B[43m        \u001B[49m\u001B[43mofficial_model_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    720\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhuggingface_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    721\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    723\u001B[0m     architecture \u001B[38;5;241m=\u001B[39m hf_config\u001B[38;5;241m.\u001B[39marchitectures[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    724\u001B[0m \u001B[38;5;66;03m# !!!\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:972\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    969\u001B[0m trust_remote_code \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrust_remote_code\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    970\u001B[0m code_revision \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcode_revision\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 972\u001B[0m config_dict, unused_kwargs \u001B[38;5;241m=\u001B[39m \u001B[43mPretrainedConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_config_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    973\u001B[0m has_remote_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutoConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    974\u001B[0m has_local_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m CONFIG_MAPPING\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:632\u001B[0m, in \u001B[0;36mPretrainedConfig.get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    630\u001B[0m original_kwargs \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(kwargs)\n\u001B[1;32m    631\u001B[0m \u001B[38;5;66;03m# Get config dict associated with the base config file\u001B[39;00m\n\u001B[0;32m--> 632\u001B[0m config_dict, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_config_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict:\n\u001B[1;32m    634\u001B[0m     original_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001B[0m, in \u001B[0;36mPretrainedConfig._get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    685\u001B[0m configuration_file \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_configuration_file\u001B[39m\u001B[38;5;124m\"\u001B[39m, CONFIG_NAME) \u001B[38;5;28;01mif\u001B[39;00m gguf_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m gguf_file\n\u001B[1;32m    687\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    688\u001B[0m     \u001B[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001B[39;00m\n\u001B[0;32m--> 689\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    690\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    691\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfiguration_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    692\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    693\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    694\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    695\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    696\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    698\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    699\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    700\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    701\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    703\u001B[0m     commit_hash \u001B[38;5;241m=\u001B[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001B[39;00m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;66;03m# the original exception.\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:445\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    440\u001B[0m         resolved_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    441\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_missing_entries\n\u001B[1;32m    442\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_connection_errors\n\u001B[1;32m    443\u001B[0m     ):\n\u001B[1;32m    444\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m resolved_file\n\u001B[0;32m--> 445\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    446\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe couldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt connect to \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to load this file, couldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find it in the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    447\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m cached files and it looks like \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not the path to a directory containing a file named\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    448\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfull_filename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCheckout your internet connection or see how to run the library in offline mode at\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    449\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    450\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    451\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    452\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001B[0;31mOSError\u001B[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like gpt2 is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "import os\n",
    "# # Add domestic mirror path\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "# this is an outdated way to load the SAE. We need to have feature spartisity loadable through the new interface to remove it.\n",
    "gpt2_small_sparse_autoencoders = {}\n",
    "gpt2_small_sae_sparsities = {}\n",
    "\n",
    "for layer in range(12):\n",
    "    sae, original_cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gpt2-small-res-jb\",\n",
    "        sae_id=\"blocks.0.hook_resid_pre\",\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    gpt2_small_sparse_autoencoders[f\"blocks.{layer}.hook_resid_pre\"] = sae\n",
    "    gpt2_small_sae_sparsities[f\"blocks.{layer}.hook_resid_pre\"] = sparsity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pFYJKeNBSdD"
   },
   "source": [
    "# Statistical Properties of Feature Logit Distributions\n",
    "\n",
    "In the post I study layer 8 (for no particular reason). At the end of this notebook is code for visualizing these statistics across all layers. Feel free to change the layer here and explore different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEK1jEpEBSdD"
   },
   "outputs": [],
   "source": [
    "# In the post, I focus on layer 8\n",
    "layer = 8\n",
    "\n",
    "# get the corresponding SAE and feature sparsities.\n",
    "sparse_autoencoder = gpt2_small_sparse_autoencoders[f\"blocks.{layer}.hook_resid_pre\"]\n",
    "log_feature_sparsity = gpt2_small_sae_sparsities[f\"blocks.{layer}.hook_resid_pre\"].cpu()\n",
    "\n",
    "W_dec = sparse_autoencoder.W_dec.detach().cpu()\n",
    "\n",
    "# calculate the statistics of the logit weight distributions\n",
    "W_U_stats_df_dec, dec_projection_onto_W_U = get_W_U_W_dec_stats_df(\n",
    "    W_dec, model, cosine_sim=False\n",
    ")\n",
    "W_U_stats_df_dec[\"sparsity\"] = (\n",
    "    log_feature_sparsity  # add feature sparsity since it is often interesting.\n",
    ")\n",
    "display(W_U_stats_df_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GK56PDl3BSdE"
   },
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the 3rd / 4th moments. I found these aren't as useful on their own as joint distributions can be.\n",
    "px.histogram(\n",
    "    W_U_stats_df_dec,\n",
    "    x=\"skewness\",\n",
    "    width=800,\n",
    "    height=300,\n",
    "    nbins=1000,\n",
    "    title=\"Skewness of the Logit Weight Distributions\",\n",
    ").show()\n",
    "\n",
    "px.histogram(\n",
    "    W_U_stats_df_dec,\n",
    "    x=np.log10(W_U_stats_df_dec[\"kurtosis\"]),\n",
    "    width=800,\n",
    "    height=300,\n",
    "    nbins=1000,\n",
    "    title=\"Kurtosis of the Logit Weight Distributions\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_6c12ftBSdE"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    W_U_stats_df_dec,\n",
    "    x=\"skewness\",\n",
    "    y=\"kurtosis\",\n",
    "    color=\"std\",\n",
    "    color_continuous_scale=\"Portland\",\n",
    "    hover_name=\"feature\",\n",
    "    width=800,\n",
    "    height=500,\n",
    "    log_y=True,  # Kurtosis has larger outliers so logging creates a nicer scale.\n",
    "    labels={\"x\": \"Skewness\", \"y\": \"Kurtosis\", \"color\": \"Standard Deviation\"},\n",
    "    title=f\"Layer {8}: Skewness vs Kurtosis of the Logit Weight Distributions\",\n",
    ")\n",
    "\n",
    "# decrease point size\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuWSPqqjBSdE"
   },
   "outputs": [],
   "source": [
    "# then you can query accross combinations of the statistics to find features of interest and open them in neuronpedia.\n",
    "tmp_df = W_U_stats_df_dec[[\"feature\", \"skewness\", \"kurtosis\", \"std\"]]\n",
    "# tmp_df = tmp_df[(tmp_df[\"std\"] > 0.04)]\n",
    "# tmp_df = tmp_df[(tmp_df[\"skewness\"] > 0.65)]\n",
    "tmp_df = tmp_df[(tmp_df[\"skewness\"] > 3)]\n",
    "tmp_df = tmp_df.sort_values(\"skewness\", ascending=False).head(10)\n",
    "display(tmp_df)\n",
    "\n",
    "# if desired, open the features in neuronpedia\n",
    "get_neuronpedia_quick_list(list(tmp_df.feature), layer=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHGG86wkBSdE"
   },
   "source": [
    "# Token Set Enrichment Analysis\n",
    "\n",
    "We now proceed to token set enrichment analysis.  I highly recommend reading my AlignmentForum post (espeically the case studies) before reading too much into any of these results.\n",
    "Also read this [post](https://transformer-circuits.pub/2024/qualitative-essay/index.html) for good general perspectives on statistics here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4tuqT9SBSdE"
   },
   "source": [
    "## Defining Our Token Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mUxqd6mBSdE"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "# get the vocab we need to filter to formulate token sets.\n",
    "vocab = model.tokenizer.get_vocab()  # type: ignore\n",
    "\n",
    "# make a regex dictionary to specify more sets.\n",
    "regex_dict = {\n",
    "    \"starts_with_space\": r\".*\",\n",
    "    \"starts_with_capital\": r\"^*[A-Z].*\",\n",
    "    \"starts_with_lower\": r\"^*[a-z].*\",\n",
    "    \"all_digits\": r\"^*\\d+$\",\n",
    "    \"is_punctuation\": r\"^[^\\w\\s]+$\",\n",
    "    \"contains_close_bracket\": r\".*\\).*\",\n",
    "    \"contains_open_bracket\": r\".*\\(.*\",\n",
    "    \"all_caps\": r\"*[A-Z]+$\",\n",
    "    \"1 digit\": r\"*\\d{1}$\",\n",
    "    \"2 digits\": r\"*\\d{2}$\",\n",
    "    \"3 digits\": r\"*\\d{3}$\",\n",
    "    \"4 digits\": r\"*\\d{4}$\",\n",
    "    \"length_1\": r\"^*\\w{1}$\",\n",
    "    \"length_2\": r\"^*\\w{2}$\",\n",
    "    \"length_3\": r\"^*\\w{3}$\",\n",
    "    \"length_4\": r\"^*\\w{4}$\",\n",
    "    \"length_5\": r\"^*\\w{5}$\",\n",
    "}\n",
    "\n",
    "# print size of gene sets\n",
    "all_token_sets = get_letter_gene_sets(vocab)\n",
    "for key, value in regex_dict.items():\n",
    "    gene_set = get_gene_set_from_regex(vocab, value)\n",
    "    all_token_sets[key] = gene_set\n",
    "\n",
    "# some other sets that can be interesting\n",
    "baby_name_sets = get_baby_name_sets(vocab)\n",
    "pos_sets = generate_pos_sets(vocab)\n",
    "arbitrary_sets = get_test_gene_sets(model)\n",
    "\n",
    "all_token_sets = {**all_token_sets, **pos_sets}\n",
    "all_token_sets = {**all_token_sets, **arbitrary_sets}\n",
    "all_token_sets = {**all_token_sets, **baby_name_sets}\n",
    "\n",
    "# for each gene set, convert to string and  print the first 5 tokens\n",
    "for token_set_name, gene_set in sorted(\n",
    "    all_token_sets.items(), key=lambda x: len(x[1]), reverse=True\n",
    "):\n",
    "    tokens = [model.to_string(id) for id in list(gene_set)][:10]  # type: ignore\n",
    "    print(f\"{token_set_name}, has {len(gene_set)} genes\")\n",
    "    print(tokens)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxctX05KBSdE"
   },
   "source": [
    "## Performing Token Set Enrichment Analysis\n",
    "\n",
    "Below we perform token set enrichment analysis on various token sets. In practice, we'd likely perform tests accross all tokens and large libraries of sets simultaneously but to make it easier to run, we look at features with higher skew and select of a few token sets at a time to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHwM7qVlBSdF"
   },
   "outputs": [],
   "source": [
    "features_ordered_by_skew = (\n",
    "    W_U_stats_df_dec[\"skewness\"].sort_values(ascending=False).head(5000).index.to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxuiBx4NBSdF"
   },
   "outputs": [],
   "source": [
    "# filter our list.\n",
    "token_sets_index = [\n",
    "    \"starts_with_space\",\n",
    "    \"starts_with_capital\",\n",
    "    \"all_digits\",\n",
    "    \"is_punctuation\",\n",
    "    \"all_caps\",\n",
    "]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "\n",
    "# calculate the enrichment scores\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U,  # use the logit weight values as our rankings over tokens.\n",
    "    features_ordered_by_skew,  # subset by these features\n",
    "    token_set_selected,  # use token_sets\n",
    ")\n",
    "\n",
    "manhattan_plot_enrichment_scores(\n",
    "    df_enrichment_scores, label_threshold=0, top_n=3  # use our enrichment scores\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQ0n0aKTBSdF"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df_enrichment_scores.apply(lambda x: -1 * np.log(1 - x)).T,\n",
    "    x=\"starts_with_space\",\n",
    "    y=\"starts_with_capital\",\n",
    "    marginal_x=\"histogram\",\n",
    "    marginal_y=\"histogram\",\n",
    "    labels={\n",
    "        \"starts_with_space\": \"Starts with Space\",\n",
    "        \"starts_with_capital\": \"Starts with Capital\",\n",
    "    },\n",
    "    title=\"Enrichment Scores for Starts with Space vs Starts with Capital\",\n",
    "    height=800,\n",
    "    width=800,\n",
    ")\n",
    "# reduce point size on the scatter only\n",
    "fig.update_traces(marker=dict(size=2), selector=dict(mode=\"markers\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcmU_6I9BSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"1 digit\", \"2 digits\", \"3 digits\", \"4 digits\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BboYni5ZBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"nltk_pos_PRP\", \"nltk_pos_VBZ\", \"nltk_pos_NNP\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKrcnE7GBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"nltk_pos_VBN\", \"nltk_pos_VBG\", \"nltk_pos_VB\", \"nltk_pos_VBD\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKVdyyxoBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"nltk_pos_WP\", \"nltk_pos_RBR\", \"nltk_pos_WDT\", \"nltk_pos_RB\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYC3GFscBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"a\", \"e\", \"i\", \"o\", \"u\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvmgQ_YmBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"negative_words\", \"positive_words\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltYmy5lMBSdF"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df_enrichment_scores.apply(lambda x: -1 * np.log(1 - x))\n",
    "    .T.reset_index()\n",
    "    .rename(columns={\"index\": \"feature\"}),\n",
    "    x=\"negative_words\",\n",
    "    y=\"positive_words\",\n",
    "    marginal_x=\"histogram\",\n",
    "    marginal_y=\"histogram\",\n",
    "    labels={\n",
    "        \"starts_with_space\": \"Starts with Space\",\n",
    "        \"starts_with_capital\": \"Starts with Capital\",\n",
    "    },\n",
    "    title=\"Enrichment Scores for Starts with Space vs Starts with Capital\",\n",
    "    height=800,\n",
    "    width=800,\n",
    "    hover_name=\"feature\",\n",
    ")\n",
    "# reduce point size on the scatter only\n",
    "fig.update_traces(marker=dict(size=2), selector=dict(mode=\"markers\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5otoyu2SBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"contains_close_bracket\", \"contains_open_bracket\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlN-ScWhBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\n",
    "    \"1910's\",\n",
    "    \"1920's\",\n",
    "    \"1930's\",\n",
    "    \"1940's\",\n",
    "    \"1950's\",\n",
    "    \"1960's\",\n",
    "    \"1970's\",\n",
    "    \"1980's\",\n",
    "    \"1990's\",\n",
    "    \"2000's\",\n",
    "    \"2010's\",\n",
    "]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDmCs9OhBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"positive_words\", \"negative_words\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores, label_threshold=0.98).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilzC33VlBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"boys_names\", \"girls_names\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0mTCic7BSdG"
   },
   "outputs": [],
   "source": [
    "tmp_df = df_enrichment_scores.apply(lambda x: -1 * np.log(1 - x)).T\n",
    "color = (\n",
    "    W_U_stats_df_dec.sort_values(\"skewness\", ascending=False)\n",
    "    .head(5000)[\"skewness\"]\n",
    "    .values\n",
    ")\n",
    "fig = px.scatter(\n",
    "    tmp_df.reset_index().rename(columns={\"index\": \"feature\"}),\n",
    "    x=\"boys_names\",\n",
    "    y=\"girls_names\",\n",
    "    marginal_x=\"histogram\",\n",
    "    marginal_y=\"histogram\",\n",
    "    # color = color,\n",
    "    labels={\n",
    "        \"boys_names\": \"Enrichment Score (Boys Names)\",\n",
    "        \"girls_names\": \"Enrichment Score (Girls Names)\",\n",
    "    },\n",
    "    height=600,\n",
    "    width=800,\n",
    "    hover_name=\"feature\",\n",
    ")\n",
    "# reduce point size on the scatter only\n",
    "fig.update_traces(marker=dict(size=3), selector=dict(mode=\"markers\"))\n",
    "# annotate any features where the absolute distance between boys names and girls names > 3\n",
    "for feature in df_enrichment_scores.columns:\n",
    "    if abs(tmp_df[\"boys_names\"][feature] - tmp_df[\"girls_names\"][feature]) > 2.9:\n",
    "        fig.add_annotation(\n",
    "            x=tmp_df[\"boys_names\"][feature] - 0.4,\n",
    "            y=tmp_df[\"girls_names\"][feature] + 0.1,\n",
    "            text=f\"{feature}\",\n",
    "            showarrow=False,\n",
    "        )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwVcsxnkBSdG"
   },
   "source": [
    "## Digging into Particular Features\n",
    "\n",
    "When we do these enrichments, I generate the logit weight histograms by category using the following function. It's important to make sure the categories you group by are in the columns of df_enrichment_scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgndTFdFBSdG"
   },
   "outputs": [],
   "source": [
    "for category in [\"boys_names\"]:\n",
    "    plot_top_k_feature_projections_by_token_and_category(\n",
    "        token_set_selected,\n",
    "        df_enrichment_scores,\n",
    "        category=category,\n",
    "        dec_projection_onto_W_U=dec_projection_onto_W_U,\n",
    "        model=model,\n",
    "        log_y=False,\n",
    "        histnorm=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKP3u0D2BSdG"
   },
   "source": [
    "# Appendix Results: Logit Weight distribution Statistics Accross All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vhht2iCQBSdG"
   },
   "outputs": [],
   "source": [
    "W_U_stats_df_dec_all_layers = get_all_stats_dfs(\n",
    "    gpt2_small_sparse_autoencoders, gpt2_small_sae_sparsities, model, cosine_sim=True\n",
    ")\n",
    "\n",
    "display(W_U_stats_df_dec_all_layers.shape)\n",
    "display(W_U_stats_df_dec_all_layers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ok3DgORLBSdG"
   },
   "outputs": [],
   "source": [
    "# Let's plot the percentiles of the skewness and kurtosis by layer\n",
    "tmp_df = W_U_stats_df_dec_all_layers.groupby(\"layer\")[\"skewness\"].describe(\n",
    "    percentiles=[0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    ")\n",
    "tmp_df = tmp_df[[\"1%\", \"5%\", \"10%\", \"25%\", \"50%\", \"75%\", \"90%\", \"95%\", \"99%\"]]\n",
    "\n",
    "fig = px.area(\n",
    "    tmp_df,\n",
    "    title=\"Skewness by Layer\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    color_discrete_sequence=px.colors.sequential.Turbo,\n",
    ").show()\n",
    "\n",
    "\n",
    "tmp_df = W_U_stats_df_dec_all_layers.groupby(\"layer\")[\"kurtosis\"].describe(\n",
    "    percentiles=[0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    ")\n",
    "tmp_df = tmp_df[[\"1%\", \"5%\", \"10%\", \"25%\", \"50%\", \"75%\", \"90%\", \"95%\", \"99%\"]]\n",
    "\n",
    "fig = px.area(\n",
    "    tmp_df,\n",
    "    title=\"Kurtosis by Layer\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    color_discrete_sequence=px.colors.sequential.Turbo,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOkRZiunBSdG"
   },
   "outputs": [],
   "source": [
    "# let's make a pretty color scheme\n",
    "from plotly.colors import n_colors\n",
    "\n",
    "colors = n_colors(\"rgb(5, 200, 200)\", \"rgb(200, 10, 10)\", 13, colortype=\"rgb\")\n",
    "\n",
    "# Make a box plot of the skewness by layer\n",
    "fig = px.box(\n",
    "    W_U_stats_df_dec_all_layers,\n",
    "    x=\"layer\",\n",
    "    y=\"skewness\",\n",
    "    color=\"layer\",\n",
    "    color_discrete_sequence=colors,\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    title=\"Skewness cos(W_U,W_dec) by Layer in GPT2 Small Residual Stream SAEs\",\n",
    "    labels={\"layer\": \"Layer\", \"skewnss\": \"Skewness\"},\n",
    ")\n",
    "fig.update_xaxes(showticklabels=True, dtick=1)\n",
    "\n",
    "# increase font size\n",
    "fig.update_layout(font=dict(size=16))\n",
    "fig.show()\n",
    "\n",
    "# Make a box plot of the skewness by layer\n",
    "fig = px.box(\n",
    "    W_U_stats_df_dec_all_layers,\n",
    "    x=\"layer\",\n",
    "    y=\"kurtosis\",\n",
    "    color=\"layer\",\n",
    "    color_discrete_sequence=colors,\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    log_y=True,\n",
    "    title=\"log kurtosis cos(W_U,W_dec) by Layer in GPT2 Small Residual Stream SAEs\",\n",
    "    labels={\"layer\": \"Layer\", \"kurtosis\": \"Log Kurtosis\"},\n",
    ")\n",
    "fig.update_xaxes(showticklabels=True, dtick=1)\n",
    "\n",
    "# increase font size\n",
    "fig.update_layout(font=dict(size=16))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYNYdY3wBSdG"
   },
   "outputs": [],
   "source": [
    "# scatter\n",
    "fig = px.scatter(\n",
    "    W_U_stats_df_dec_all_layers[W_U_stats_df_dec_all_layers.log_feature_sparsity >= -9],\n",
    "    # W_U_stats_df_dec_all_layers[W_U_stats_df_dec_all_layers.layer == 8],\n",
    "    x=\"skewness\",\n",
    "    y=\"kurtosis\",\n",
    "    color=\"std\",\n",
    "    color_continuous_scale=\"Portland\",\n",
    "    hover_name=\"feature\",\n",
    "    # color_continuous_midpoint = 0,\n",
    "    # range_color = [-4,-1],\n",
    "    log_y=True,\n",
    "    height=800,\n",
    "    # width = 2000,\n",
    "    # facet_col=\"layer\",\n",
    "    # facet_col_wrap=5,\n",
    "    animation_frame=\"layer\",\n",
    ")\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.for_each_yaxis(lambda yaxis: yaxis.update(showticklabels=True))\n",
    "\n",
    "# decrease point size\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.show()\n",
    "fig.write_html(\"skewness_kurtosis_scatter_all_layers.html\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
